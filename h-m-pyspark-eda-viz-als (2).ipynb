{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-30T02:58:59.944072Z","iopub.execute_input":"2022-04-30T02:58:59.944540Z","iopub.status.idle":"2022-04-30T02:59:41.798987Z","shell.execute_reply.started":"2022-04-30T02:58:59.944428Z","shell.execute_reply":"2022-04-30T02:59:41.797895Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## EDA of the Data with PySpark SQL Library \n## Recommendations and Machine Learning using Pyspark ML","metadata":{}},{"cell_type":"markdown","source":"<center><h3>The Data is fairly big 3.50 GB and will need Faster Computation by having it in the memory</center>","metadata":{}},{"cell_type":"code","source":"# Installing PySpark\n!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:00:17.687266Z","iopub.execute_input":"2022-04-30T03:00:17.687632Z","iopub.status.idle":"2022-04-30T03:01:20.394055Z","shell.execute_reply.started":"2022-04-30T03:00:17.687597Z","shell.execute_reply":"2022-04-30T03:01:20.392624Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Some other Dependencies\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport os \nimport re\nimport json\nfrom os import listdir\nfrom os.path import isfile, join\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:01:21.895345Z","iopub.execute_input":"2022-04-30T03:01:21.895618Z","iopub.status.idle":"2022-04-30T03:01:21.905905Z","shell.execute_reply.started":"2022-04-30T03:01:21.895588Z","shell.execute_reply":"2022-04-30T03:01:21.904790Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Reading in csv to do some EDA / Training with them \narticles_df=pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\ncustomers_df=pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/customers.csv')\ntransactions_df=pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:01:25.558609Z","iopub.execute_input":"2022-04-30T03:01:25.559134Z","iopub.status.idle":"2022-04-30T03:02:45.906177Z","shell.execute_reply.started":"2022-04-30T03:01:25.559059Z","shell.execute_reply":"2022-04-30T03:02:45.904273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Checking the Dataset in the Tabular Form\nprint(articles_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:02:50.901818Z","iopub.execute_input":"2022-04-30T03:02:50.902284Z","iopub.status.idle":"2022-04-30T03:02:50.943692Z","shell.execute_reply.started":"2022-04-30T03:02:50.902236Z","shell.execute_reply":"2022-04-30T03:02:50.942853Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(customers_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:02:54.826041Z","iopub.execute_input":"2022-04-30T03:02:54.827084Z","iopub.status.idle":"2022-04-30T03:02:54.855808Z","shell.execute_reply.started":"2022-04-30T03:02:54.827036Z","shell.execute_reply":"2022-04-30T03:02:54.854642Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(transactions_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:02:56.900453Z","iopub.execute_input":"2022-04-30T03:02:56.900866Z","iopub.status.idle":"2022-04-30T03:02:56.910906Z","shell.execute_reply.started":"2022-04-30T03:02:56.900818Z","shell.execute_reply":"2022-04-30T03:02:56.909646Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Spark Session","metadata":{}},{"cell_type":"code","source":"# Creating a Spark Session\nimport pyspark\nfrom pyspark.sql import SparkSession\nsc=SparkSession.builder.appName('H-M-Recommender-Systesm').config(\"spark.sql.files.maxPartitionBytes\",10000000).getOrCreate()\nspark=SparkSession(sc)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-30T03:02:59.852228Z","iopub.execute_input":"2022-04-30T03:02:59.852773Z","iopub.status.idle":"2022-04-30T03:03:07.585620Z","shell.execute_reply.started":"2022-04-30T03:02:59.852735Z","shell.execute_reply":"2022-04-30T03:03:07.584711Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Files into the Spark DataFrames","metadata":{}},{"cell_type":"code","source":"# Importing the data into the Spark DataFrames to do the operations\n# articles=spark.read.format(\"csv\").load('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\n# customers=spark.read.format(\"csv\").load('../input/h-and-m-personalized-fashion-recommendations/customers.csv')\n# transactions=spark.read.format(\"csv\").load('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\n# Read with options as we need the Header names as columns\ntransactions = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\narticles = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\ncustomers = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/customers.csv')\n# df.printSchema()\narticles.printSchema()\ncustomers.printSchema()\ntransactions.printSchema()\n# Here all the schemas of them will be printed","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-30T03:03:31.583373Z","iopub.execute_input":"2022-04-30T03:03:31.583789Z","iopub.status.idle":"2022-04-30T03:03:37.854284Z","shell.execute_reply.started":"2022-04-30T03:03:31.583740Z","shell.execute_reply":"2022-04-30T03:03:37.853197Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Checking the data\ntransactions.show(5)\narticles.show(5)\ncustomers.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:03:52.314428Z","iopub.execute_input":"2022-04-30T03:03:52.314820Z","iopub.status.idle":"2022-04-30T03:03:53.159891Z","shell.execute_reply.started":"2022-04-30T03:03:52.314776Z","shell.execute_reply":"2022-04-30T03:03:53.159047Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Importing Spark Libraries","metadata":{}},{"cell_type":"code","source":"import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:22.481042Z","iopub.execute_input":"2022-04-30T03:04:22.481517Z","iopub.status.idle":"2022-04-30T03:04:22.577399Z","shell.execute_reply.started":"2022-04-30T03:04:22.481473Z","shell.execute_reply":"2022-04-30T03:04:22.576380Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Create them as Spark Tables\n\nRegisters this DataFrame as a temporary table using the given name.\n\nThe lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n\nThis tables will be useful as we will be able to perform SQL Like Queries on them with ease","metadata":{}},{"cell_type":"code","source":"transactions.createOrReplaceTempView('transactions')\narticles.createOrReplaceTempView('articles')\ncustomers.createOrReplaceTempView('customers')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:28.468215Z","iopub.execute_input":"2022-04-30T03:04:28.468933Z","iopub.status.idle":"2022-04-30T03:04:28.583052Z","shell.execute_reply.started":"2022-04-30T03:04:28.468869Z","shell.execute_reply":"2022-04-30T03:04:28.581980Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Starting off with a Simple SQL Query of Selecting everything from the Table\n\n","metadata":{}},{"cell_type":"code","source":"query_1 = spark.sql('''\nselect * \nfrom transactions\nlimit 100\n''')\n\nquery_1.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:36.685233Z","iopub.execute_input":"2022-04-30T03:04:36.685574Z","iopub.status.idle":"2022-04-30T03:04:37.040355Z","shell.execute_reply.started":"2022-04-30T03:04:36.685542Z","shell.execute_reply":"2022-04-30T03:04:37.039139Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"query_2 = spark.sql('''\nselect * \nfrom customers\nlimit 100\n''')\n\nquery_2.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:40.194495Z","iopub.execute_input":"2022-04-30T03:04:40.194825Z","iopub.status.idle":"2022-04-30T03:04:40.362214Z","shell.execute_reply.started":"2022-04-30T03:04:40.194792Z","shell.execute_reply":"2022-04-30T03:04:40.361136Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"query_3 = spark.sql('''\nselect * \nfrom articles\nlimit 100\n''')\n\nquery_3.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:42.828684Z","iopub.execute_input":"2022-04-30T03:04:42.829049Z","iopub.status.idle":"2022-04-30T03:04:43.078554Z","shell.execute_reply.started":"2022-04-30T03:04:42.829014Z","shell.execute_reply":"2022-04-30T03:04:43.077546Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Aggregating to get some statistics ","metadata":{}},{"cell_type":"code","source":"# Some statistics from the Transaction Tables\nquery_4= spark.sql('''\nselect\n  customer_id\n  , collect_list(article_id) as article_id_list\n  , avg(price) as avg_price\n  , max(price) as max_price\n  , min(price) as min_price\nfrom transactions\ngroup by customer_id\nlimit 100\n''')\n\nquery_4.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:04:57.775540Z","iopub.execute_input":"2022-04-30T03:04:57.776068Z","iopub.status.idle":"2022-04-30T03:06:30.088569Z","shell.execute_reply.started":"2022-04-30T03:04:57.776021Z","shell.execute_reply":"2022-04-30T03:06:30.087159Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Getting the Number of Customers below the Age of 25","metadata":{}},{"cell_type":"code","source":"query_5=spark.sql('''\nselect count(customer_id)\nfrom \ncustomers \nwhere (customers.age<25) \n''')\nquery_5.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:06:34.789583Z","iopub.execute_input":"2022-04-30T03:06:34.789957Z","iopub.status.idle":"2022-04-30T03:06:37.677627Z","shell.execute_reply.started":"2022-04-30T03:06:34.789917Z","shell.execute_reply":"2022-04-30T03:06:37.676505Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### List of Customers that  are Active Club Members","metadata":{}},{"cell_type":"code","source":"query_6=spark.sql('''\nselect count(customer_id)\nfrom customers\nwhere customers.club_member_status=='ACTIVE'\n''')\nquery_6.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:06:45.483272Z","iopub.execute_input":"2022-04-30T03:06:45.483643Z","iopub.status.idle":"2022-04-30T03:06:47.550327Z","shell.execute_reply.started":"2022-04-30T03:06:45.483607Z","shell.execute_reply":"2022-04-30T03:06:47.548753Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Count of Customers that Follow the Fashion News Regularly","metadata":{}},{"cell_type":"code","source":"query_7=spark.sql('''\nselect fashion_news_frequency,count(customer_id)\nfrom customers\nGROUP BY fashion_news_frequency\n''')\nquery_7.show()\nq7=query_7.toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:06:56.033904Z","iopub.execute_input":"2022-04-30T03:06:56.034302Z","iopub.status.idle":"2022-04-30T03:07:00.586832Z","shell.execute_reply.started":"2022-04-30T03:06:56.034258Z","shell.execute_reply":"2022-04-30T03:07:00.585886Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the Bar Plot for this query","metadata":{}},{"cell_type":"code","source":"f, (ax1) = plt.subplots(1, 1, figsize=(6, 6), sharex=True)\nsns.barplot(x=q7['fashion_news_frequency'], y=q7['count(customer_id)'], palette=\"rocket\", ax=ax1)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:07:05.052587Z","iopub.execute_input":"2022-04-30T03:07:05.053280Z","iopub.status.idle":"2022-04-30T03:07:05.339254Z","shell.execute_reply.started":"2022-04-30T03:07:05.053235Z","shell.execute_reply":"2022-04-30T03:07:05.336877Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Key Customers - Customers that are Active , Follow Fashion News and club members","metadata":{}},{"cell_type":"code","source":"query_8=spark.sql('''\nselect customer_id \nfrom customers \nwhere\ncustomers.Active=1.0 AND\ncustomers.fashion_news_frequency=='Regularly' AND\ncustomers.club_member_status=='ACTIVE'\n\n''')\nquery_8.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:07:44.889578Z","iopub.execute_input":"2022-04-30T03:07:44.889952Z","iopub.status.idle":"2022-04-30T03:07:45.096511Z","shell.execute_reply.started":"2022-04-30T03:07:44.889914Z","shell.execute_reply":"2022-04-30T03:07:45.095227Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.246093Z","iopub.status.idle":"2022-04-25T18:00:49.246401Z","shell.execute_reply.started":"2022-04-25T18:00:49.246238Z","shell.execute_reply":"2022-04-25T18:00:49.246254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Queries on Articles Table ","metadata":{}},{"cell_type":"code","source":"# Printing the Dataframe to get an Idea  of the Columns\narticles_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.249756Z","iopub.status.idle":"2022-04-25T18:00:49.250321Z","shell.execute_reply.started":"2022-04-25T18:00:49.250147Z","shell.execute_reply":"2022-04-25T18:00:49.250166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get a List of Product Names","metadata":{}},{"cell_type":"code","source":"query_9=spark.sql('''\nselect DISTINCT prod_name\nfrom articles \n\n''')\nquery_9.show(50)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.251094Z","iopub.status.idle":"2022-04-25T18:00:49.251644Z","shell.execute_reply.started":"2022-04-25T18:00:49.251455Z","shell.execute_reply":"2022-04-25T18:00:49.251475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get a List of Type of Products","metadata":{}},{"cell_type":"code","source":"query_10=spark.sql('''\nselect DISTINCT product_type_name\nfrom articles\n''')\nquery_10.show(100)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.252629Z","iopub.status.idle":"2022-04-25T18:00:49.253288Z","shell.execute_reply.started":"2022-04-25T18:00:49.253095Z","shell.execute_reply":"2022-04-25T18:00:49.253116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get a List of Index Names","metadata":{}},{"cell_type":"code","source":"query_11=spark.sql('''\nselect DISTINCT index_name\nfrom articles\n''')\nquery_11.show(20)\n# Here we can see these are the Broad 5 Categories","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.254265Z","iopub.status.idle":"2022-04-25T18:00:49.254702Z","shell.execute_reply.started":"2022-04-25T18:00:49.254538Z","shell.execute_reply":"2022-04-25T18:00:49.254555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get a List of Index group Names","metadata":{}},{"cell_type":"code","source":"query_12=spark.sql('''\nselect DISTINCT\nindex_group_name\nfrom articles\n''')\nquery_12.show(20)\n# Here we can see these are the Broad 5 Categories","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.25583Z","iopub.status.idle":"2022-04-25T18:00:49.256476Z","shell.execute_reply.started":"2022-04-25T18:00:49.256281Z","shell.execute_reply":"2022-04-25T18:00:49.256301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_12b=spark.sql('''\nselect index_group_name,count(index_group_name)\nfrom\narticles\nGROUP BY index_group_name\n''')\nquery_12b=query_12b.toPandas()\nsns.barplot(x=\"count(index_group_name)\", y=\"index_group_name\", data=query_12b, color=\"c\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.257578Z","iopub.status.idle":"2022-04-25T18:00:49.257897Z","shell.execute_reply.started":"2022-04-25T18:00:49.257733Z","shell.execute_reply":"2022-04-25T18:00:49.257749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nquery_12a=spark.sql('''\nselect garment_group_no,count(garment_group_no)\nfrom\narticles\nGROUP BY garment_group_no\n''')\nquery_12a=query_12a.toPandas()\nsns.barplot(x=\"count(garment_group_no)\", y=\"garment_group_no\", data=query_12a, color=\"b\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.259372Z","iopub.status.idle":"2022-04-25T18:00:49.260071Z","shell.execute_reply.started":"2022-04-25T18:00:49.25985Z","shell.execute_reply":"2022-04-25T18:00:49.259872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Which Index Items are Maximum Present","metadata":{}},{"cell_type":"code","source":"query_13=spark.sql('''\nselect count(article_id)\nfrom\narticles\nGROUP BY index_group_name\n''')\nquery_13.show()\n# This shows which Index Group Name has what Number of Items Present in it","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.261048Z","iopub.status.idle":"2022-04-25T18:00:49.261704Z","shell.execute_reply.started":"2022-04-25T18:00:49.261507Z","shell.execute_reply":"2022-04-25T18:00:49.261528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sort Which Index has the Most Number of Items","metadata":{}},{"cell_type":"code","source":"query_14=spark.sql('''\nselect index_name, count (article_id)\nfrom \narticles\nGROUP BY index_name\n''')\nquery_14.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.262556Z","iopub.status.idle":"2022-04-25T18:00:49.263316Z","shell.execute_reply.started":"2022-04-25T18:00:49.263106Z","shell.execute_reply":"2022-04-25T18:00:49.263128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the Number of Each Products Present","metadata":{}},{"cell_type":"code","source":"# Get a List of Products that are Most in Number\nquery_15=spark.sql('''\nselect count(prod_name),prod_name\nfrom \narticles\nGROUP BY prod_name\nORDER BY count(prod_name)\nDESC\n''')\nquery_15.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.264244Z","iopub.status.idle":"2022-04-25T18:00:49.264829Z","shell.execute_reply.started":"2022-04-25T18:00:49.264646Z","shell.execute_reply":"2022-04-25T18:00:49.264672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Query to Find out which colour Products are present most in the Inventory","metadata":{}},{"cell_type":"code","source":"query_16=spark.sql('''\nselect count(colour_group_name), colour_group_name\nfrom\narticles\nGROUP BY colour_group_name\nORDER BY count(colour_group_name)\nDESC\n''')\nquery_16.show(20)\n# From this we can know which colours are present in what quantity in the Inventory","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.266066Z","iopub.status.idle":"2022-04-25T18:00:49.266693Z","shell.execute_reply.started":"2022-04-25T18:00:49.266485Z","shell.execute_reply":"2022-04-25T18:00:49.266509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Queries on the Transaction Table + Joins to Perform Complex Queries","metadata":{}},{"cell_type":"markdown","source":"### Printing the Transaction Table to familiarize  with the Features","metadata":{}},{"cell_type":"code","source":"transactions_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.267964Z","iopub.status.idle":"2022-04-25T18:00:49.268343Z","shell.execute_reply.started":"2022-04-25T18:00:49.26817Z","shell.execute_reply":"2022-04-25T18:00:49.268187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the Count of Number of Transactions through various Sales Id","metadata":{}},{"cell_type":"code","source":"query_17=spark.sql('''\nselect count(sales_channel_id), sales_channel_id\nfrom transactions\nGROUP BY sales_channel_id\nORDER BY count(sales_channel_id)\nDESC\n''')\nquery_17.show()\n# There are only two sales channels and these are the counts corresponding to the sales via each of em","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.269892Z","iopub.status.idle":"2022-04-25T18:00:49.270674Z","shell.execute_reply.started":"2022-04-25T18:00:49.270462Z","shell.execute_reply":"2022-04-25T18:00:49.270486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the List of Customers who have done most Number of Purchases","metadata":{}},{"cell_type":"code","source":"query_18=spark.sql('''\nselect customer_id , count(customer_id)\nfrom transactions \nGROUP BY customer_id\nORDER BY count(customer_id) DESC\n\n''')\nquery_18.show()\n# These are the customers who have purchased the Maximum amount of Items","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.271681Z","iopub.status.idle":"2022-04-25T18:00:49.272205Z","shell.execute_reply.started":"2022-04-25T18:00:49.272008Z","shell.execute_reply":"2022-04-25T18:00:49.272032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the List of Customers who have done Maximum Amount worth of Purchases","metadata":{}},{"cell_type":"code","source":"query_20=spark.sql('''\nselect customer_id, sum(price)\nfrom \ntransactions\nGROUP BY transactions.customer_id\nORDER BY (SUM(price)) \nDESC\n\n''')\nquery_20.show()\n# These are the customers that have done the most Amount worth of purchasing from the Store ","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.273085Z","iopub.status.idle":"2022-04-25T18:00:49.274097Z","shell.execute_reply.started":"2022-04-25T18:00:49.273871Z","shell.execute_reply":"2022-04-25T18:00:49.273893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What are the Items that are Most sold by H&M","metadata":{}},{"cell_type":"code","source":"query_21=spark.sql('''\nselect\n*\nfrom\ntransactions as t\ninner join articles as ar on ar.article_id = t.article_id\n''')\nquery_21.show(9)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.275259Z","iopub.status.idle":"2022-04-25T18:00:49.275585Z","shell.execute_reply.started":"2022-04-25T18:00:49.275422Z","shell.execute_reply":"2022-04-25T18:00:49.275439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = spark.sql('''\nselect\n  * \nfrom transactions as tt\nleft join customers as cs on cs.customer_id = tt.customer_id\nleft join articles as ar on ar.article_id = tt.article_id\nlimit 100\n''')\n\nprint(merged.count())\nprint(merged.columns)\n\n\ndf_merged = merged.toPandas()\n\nprint(df_merged.shape)\ndf_merged.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T18:00:49.277241Z","iopub.status.idle":"2022-04-25T18:00:49.277562Z","shell.execute_reply.started":"2022-04-25T18:00:49.277403Z","shell.execute_reply":"2022-04-25T18:00:49.277419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building an ALS Recommender with Pyspark","metadata":{}},{"cell_type":"markdown","source":"Collaborative filtering: Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users. Consider example if a person A likes item 1, 2, 3 and B like 2,3,4 then they have similar interests and A should like item 4 and B should like item 1.\n\nAlternating least square(ALS) matrix factorization: The idea is basically to take a large (or potentially huge) matrix and factor it into some smaller representation of the original matrix through alternating least squares. We end up with two or more lower dimensional matrices whose product equals the original one.ALS comes inbuilt in Apache Spark.","metadata":{}},{"cell_type":"markdown","source":"### Importing some dependencies","metadata":{}},{"cell_type":"code","source":"# Importing some dependencies \nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType \nfrom pyspark.sql.types import ArrayType, DoubleType, BooleanType\nfrom pyspark.sql.functions import col,array_contains\nfrom pyspark.sql import SQLContext \nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql.functions import udf,col,when\nfrom pyspark.sql.functions import to_timestamp,date_format\nimport numpy as np\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import *\nfrom pyspark.sql.functions import min, max\nfrom pyspark.sql.functions import unix_timestamp, lit\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:34:56.056887Z","iopub.execute_input":"2022-04-30T03:34:56.057762Z","iopub.status.idle":"2022-04-30T03:34:56.068938Z","shell.execute_reply.started":"2022-04-30T03:34:56.057703Z","shell.execute_reply":"2022-04-30T03:34:56.067362Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Reading Once again in Spark DataFrames","metadata":{}},{"cell_type":"code","source":"#  Reading them in PySpark DataFrames\ntransactionsr = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\narticlesr = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\ncustomersr = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/customers.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:35:28.933317Z","iopub.execute_input":"2022-04-30T03:35:28.933722Z","iopub.status.idle":"2022-04-30T03:35:29.700592Z","shell.execute_reply.started":"2022-04-30T03:35:28.933679Z","shell.execute_reply":"2022-04-30T03:35:29.699826Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"min_date, max_date = transactionsr.select(min(\"t_dat\"), max(\"t_dat\")).first()\nmin_date, max_date","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:35:36.628253Z","iopub.execute_input":"2022-04-30T03:35:36.628633Z","iopub.status.idle":"2022-04-30T03:36:14.506057Z","shell.execute_reply.started":"2022-04-30T03:35:36.628589Z","shell.execute_reply":"2022-04-30T03:36:14.505309Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Segregatng the DateTime column to extract fields of the date time","metadata":{}},{"cell_type":"code","source":"# Preparing the Datetime dataset for the Recommender System\nhm =  transactionsr.withColumn('t_dat', transactionsr['t_dat'].cast('string'))\nhm = hm.withColumn('date', from_unixtime(unix_timestamp('t_dat', 'yyyy-MM-dd')))\nhm = hm.withColumn('year', year(col('date')))\nhm = hm.withColumn('month', month(col('date')))\nhm = hm.withColumn('day', date_format(col('date'), \"d\"))\n\nhm = hm[hm['year'] == 2020]\nhm = hm[hm['month'] == 9]\nhm = hm[hm['day'] == 22]\ntransactionsr.unpersist()\n\n# Prepare the dataset\nhm = hm.groupby('customer_id', 'article_id').count()\nhm.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:36:44.033049Z","iopub.execute_input":"2022-04-30T03:36:44.033456Z","iopub.status.idle":"2022-04-30T03:38:02.573262Z","shell.execute_reply.started":"2022-04-30T03:36:44.033413Z","shell.execute_reply":"2022-04-30T03:38:02.572178Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Getting the total Number of Articles and users and making the sparsity","metadata":{}},{"cell_type":"code","source":"# Count the total number of article count in the dataset\nnumerator = hm.select(\"count\").count()\n\n# Count the number of distinct customerid and distinct articleid\nnum_users = hm.select(\"customer_id\").distinct().count()\nnum_articles = hm.select(\"article_id\").distinct().count()\n\n# Set the denominator equal to the number of customer multiplied by the number of articles\ndenominator = num_users * num_articles\n\n# Divide the numerator by the denominator\nsparsity = (1.0 - (numerator *1.0)/denominator)*100\nprint(\"Sparsity: \", \"%.2f\" % sparsity + \"%.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:38:37.214384Z","iopub.execute_input":"2022-04-30T03:38:37.214757Z","iopub.status.idle":"2022-04-30T03:42:10.043140Z","shell.execute_reply.started":"2022-04-30T03:38:37.214713Z","shell.execute_reply":"2022-04-30T03:42:10.042187Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"userId_count = hm.groupBy(\"customer_id\").count().orderBy('count', ascending=False)\nuserId_count.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:43:44.365825Z","iopub.execute_input":"2022-04-30T03:43:44.366251Z","iopub.status.idle":"2022-04-30T03:44:55.546361Z","shell.execute_reply.started":"2022-04-30T03:43:44.366203Z","shell.execute_reply":"2022-04-30T03:44:55.545002Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"articleId_count = hm.groupBy(\"article_id\").count().orderBy('count', ascending=False)\narticleId_count.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:46:14.873500Z","iopub.execute_input":"2022-04-30T03:46:14.873894Z","iopub.status.idle":"2022-04-30T03:47:28.970282Z","shell.execute_reply.started":"2022-04-30T03:46:14.873853Z","shell.execute_reply":"2022-04-30T03:47:28.969155Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"###  Importing some Libraries","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\nindexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(hm.columns)-set(['count'])) ]\npipeline = Pipeline(stages=indexer)\ntransformed = pipeline.fit(hm).transform(hm)\ntransformed.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:50:46.779145Z","iopub.execute_input":"2022-04-30T03:50:46.779536Z","iopub.status.idle":"2022-04-30T03:54:36.885635Z","shell.execute_reply.started":"2022-04-30T03:50:46.779494Z","shell.execute_reply":"2022-04-30T03:54:36.884506Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Preparing the Dataset for Training and Test\n(training,test)=transformed.randomSplit([0.8, 0.2])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:54:47.872685Z","iopub.execute_input":"2022-04-30T03:54:47.873050Z","iopub.status.idle":"2022-04-30T03:54:47.911070Z","shell.execute_reply.started":"2022-04-30T03:54:47.873007Z","shell.execute_reply":"2022-04-30T03:54:47.910333Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Using the Inbuilt ALS Method and fitting the train data on it","metadata":{}},{"cell_type":"code","source":"als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"customer_id_index\",itemCol=\"article_id_index\",ratingCol=\"count\",coldStartStrategy=\"drop\",nonnegative=True)\nmodel=als.fit(training)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-30T03:55:28.951922Z","iopub.execute_input":"2022-04-30T03:55:28.952335Z","iopub.status.idle":"2022-04-30T03:58:21.139424Z","shell.execute_reply.started":"2022-04-30T03:55:28.952295Z","shell.execute_reply":"2022-04-30T03:58:21.138202Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating it based on the Regression Evaluator on test data","metadata":{}},{"cell_type":"code","source":"evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"count\",predictionCol=\"prediction\")\npredictions=model.transform(test)\nrmse=evaluator.evaluate(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:59:04.148730Z","iopub.execute_input":"2022-04-30T03:59:04.149127Z","iopub.status.idle":"2022-04-30T04:00:30.432621Z","shell.execute_reply.started":"2022-04-30T03:59:04.149069Z","shell.execute_reply":"2022-04-30T04:00:30.431525Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Recommend Items and Users - Collborative Filtering method","metadata":{}},{"cell_type":"code","source":"user_recs=model.recommendForAllItems(10).show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T04:09:20.303690Z","iopub.execute_input":"2022-04-30T04:09:20.304062Z","iopub.status.idle":"2022-04-30T04:09:26.865735Z","shell.execute_reply.started":"2022-04-30T04:09:20.304028Z","shell.execute_reply":"2022-04-30T04:09:26.864538Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"item_recs=model.recommendForAllUsers(10).show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T04:08:33.247475Z","iopub.execute_input":"2022-04-30T04:08:33.247806Z","iopub.status.idle":"2022-04-30T04:08:42.671390Z","shell.execute_reply.started":"2022-04-30T04:08:33.247760Z","shell.execute_reply":"2022-04-30T04:08:42.670209Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"%%time\nuserRecsDf = model.recommendForAllUsers(10).cache()\nuserRecsDf.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T04:10:50.247182Z","iopub.execute_input":"2022-04-30T04:10:50.247600Z","iopub.status.idle":"2022-04-30T04:11:01.671637Z","shell.execute_reply.started":"2022-04-30T04:10:50.247554Z","shell.execute_reply":"2022-04-30T04:11:01.670568Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"userRecsDf.select(\"customer_id_index\",\"recommendations.article_id_index\").show(10,False)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T04:14:21.651790Z","iopub.execute_input":"2022-04-30T04:14:21.652226Z","iopub.status.idle":"2022-04-30T04:14:21.845345Z","shell.execute_reply.started":"2022-04-30T04:14:21.652179Z","shell.execute_reply":"2022-04-30T04:14:21.844458Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### Testing a Few Classification Models on the Dataset","metadata":{}},{"cell_type":"code","source":"qc=spark.sql('''\nselect customer_id,fashion_news_frequency,club_member_status, Active\nfrom customers\n''')\nqc.show()\nqhm=qc.toPandas()\n\n# # Imputing and Encoding the Data for generating the Heatmap\n# qhm['club_member_status'].fillna(value='INACTIVE', inplace=True)\n# qhm['Active'].fillna(value=0, inplace=True)\n# # qhm['fashion_news_frequency'].replace(\"NONE\", \"nf\")\n# # qhm['fashion_news_frequency'].replace(\"None\", \"nf\")\n# qhm['fashion_news_frequency'].replace({'None': 'NONE'}, regex=True)\n# qhm['fashion_news_frequency'].fillna(value='NONE', inplace=True)\n# mymap = {'1.0':1}\n# qhm.applymap(lambda s: mymap.get(s) if s in mymap else s)\n# mymap = {'ACTIVE':1,'INACTIVE':0,'PRE-CREATE':2,'NONE':0,'Regularly':1,'Monthly':2,'LEFT CLUB':3,'1.0':1,'None':0}\n# qhm=qhm.applymap(lambda s: mymap.get(s) if s in mymap else s)\n# print(qhm['fashion_news_frequency'].unique())\n# print(qhm['club_member_status'].unique())\n# print(qhm['Active'].unique())\n# Label Encoding for the dataframe\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n\n# Iterative Label Encoding\n# for col in qhm.columns:\n#     print(col)\n#     le.fit(col.values())\n#     le.transform(col.values())\nqhm.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:10:41.857764Z","iopub.execute_input":"2022-04-30T03:10:41.858271Z","iopub.status.idle":"2022-04-30T03:10:55.530396Z","shell.execute_reply.started":"2022-04-30T03:10:41.858218Z","shell.execute_reply":"2022-04-30T03:10:55.529429Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Classifying based on the Features if they purchase above the Average Shopping Cart - Machine Learning at Scale","metadata":{}},{"cell_type":"markdown","source":"### Getting the Average Ticket Size","metadata":{}},{"cell_type":"code","source":"ats=spark.sql('''\nfrom transactions select avg(price)\n''')\nats.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:12:24.960897Z","iopub.execute_input":"2022-04-30T03:12:24.961945Z","iopub.status.idle":"2022-04-30T03:13:05.183299Z","shell.execute_reply.started":"2022-04-30T03:12:24.961890Z","shell.execute_reply":"2022-04-30T03:13:05.182212Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"ats=ats.toPandas()\navg_spend=ats.iloc[0,0]\navg_spend\n# ats=ats.toPandas()\n# sum_spend=0\n# n=0\n# for i in range(len(ats)):\n#     sum_spend=sum_spend+ats.iloc[i,0]\n#     n=n+1\n# avg_spend=sum_spend/","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:13:51.289558Z","iopub.execute_input":"2022-04-30T03:13:51.290039Z","iopub.status.idle":"2022-04-30T03:14:23.800661Z","shell.execute_reply.started":"2022-04-30T03:13:51.289991Z","shell.execute_reply":"2022-04-30T03:14:23.799708Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Get the Average Spending of each Customer and the Sum Total of each Customer's purchases","metadata":{}},{"cell_type":"code","source":"avg=spark.sql('''\nselect\n  customer_id,\n  avg(price) as avg_price, sum(price) as sum_price\nfrom transactions\ngroup by customer_id\n ''')\navg.show(18)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:14:31.659558Z","iopub.execute_input":"2022-04-30T03:14:31.660418Z","iopub.status.idle":"2022-04-30T03:15:31.222228Z","shell.execute_reply.started":"2022-04-30T03:14:31.660366Z","shell.execute_reply":"2022-04-30T03:15:31.221182Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Registering qc and Avg as Tables to for query operability","metadata":{}},{"cell_type":"code","source":"qc.createOrReplaceTempView('qc')\navg.createOrReplaceTempView('avg')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:15:55.819018Z","iopub.execute_input":"2022-04-30T03:15:55.819372Z","iopub.status.idle":"2022-04-30T03:15:56.035572Z","shell.execute_reply.started":"2022-04-30T03:15:55.819338Z","shell.execute_reply":"2022-04-30T03:15:56.034497Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Ineer Join on the Tables to get the DataFrame","metadata":{}},{"cell_type":"code","source":"# Inner Join\navg_spend_cust=spark.sql('''\nselect avg.customer_id,avg.sum_price,avg.avg_price,qc.fashion_news_frequency,qc.club_member_status,qc.Active\nfrom avg \nINNER JOIN\nqc\nON qc.customer_id=avg.customer_id\n\n''')\navg_spend_cust.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:16:35.875983Z","iopub.execute_input":"2022-04-30T03:16:35.876407Z","iopub.status.idle":"2022-04-30T03:17:31.836327Z","shell.execute_reply.started":"2022-04-30T03:16:35.876363Z","shell.execute_reply":"2022-04-30T03:17:31.835540Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Getting the DataFrame ready for classification by checking spend > avg or not","metadata":{}},{"cell_type":"code","source":"avg_spend_cust.createOrReplaceTempView('main')\nfrom pyspark.sql.functions import when\ndf = avg_spend_cust.withColumn(\"avg_price\", when(avg_spend_cust.avg_price >= 0.02782927385699682,1) \\\n      .otherwise(0))\ndf.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:18:38.010982Z","iopub.execute_input":"2022-04-30T03:18:38.011361Z","iopub.status.idle":"2022-04-30T03:19:29.802292Z","shell.execute_reply.started":"2022-04-30T03:18:38.011323Z","shell.execute_reply":"2022-04-30T03:19:29.801193Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"df=df.toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:23:46.374341Z","iopub.execute_input":"2022-04-30T03:23:46.374860Z","iopub.status.idle":"2022-04-30T03:24:55.991063Z","shell.execute_reply.started":"2022-04-30T03:23:46.374814Z","shell.execute_reply":"2022-04-30T03:24:55.989934Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Imputing the Data","metadata":{}},{"cell_type":"code","source":"# Imputing and Encoding the Data for generating the Heatmap\ndf['club_member_status'].fillna(value='INACTIVE', inplace=True)\n# df['Active'].fillna(value=0, inplace=True)\n# qhm['fashion_news_frequency'].replace(\"NONE\", \"nf\")\n# qhm['fashion_news_frequency'].replace(\"None\", \"nf\")\ndf['fashion_news_frequency'].replace({'None': 'NONE'}, regex=True)\ndf['fashion_news_frequency'].fillna(value='NONE', inplace=True)\n# mymap = {'ACTIVE':1,'INACTIVE':0,'PRE-CREATE':2,'NONE':0,'Regularly':1,'Monthly':2,'LEFT CLUB':3,'1.0':1,'None':0}\n# df=df.applymap(lambda s: mymap.get(s) if s in mymap else s)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:25:11.039064Z","iopub.execute_input":"2022-04-30T03:25:11.039564Z","iopub.status.idle":"2022-04-30T03:25:13.823711Z","shell.execute_reply.started":"2022-04-30T03:25:11.039495Z","shell.execute_reply":"2022-04-30T03:25:13.822596Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Check Categorical Data For Label Encoding","metadata":{}},{"cell_type":"code","source":"print(df['fashion_news_frequency'].unique())\nprint(df['club_member_status'].unique())\nprint(df['Active'].unique())\ndf.head()\nX=df.drop(['customer_id','avg_price'], axis = 1)\ny=df['avg_price']\n# from sklearn.model_selection import train_test_split\n# # X=df['sum_price','fashion_news_frequency','club_member_status','Active']\n# df['avg_price']\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:26:33.012951Z","iopub.execute_input":"2022-04-30T03:26:33.013356Z","iopub.status.idle":"2022-04-30T03:26:33.992321Z","shell.execute_reply.started":"2022-04-30T03:26:33.013311Z","shell.execute_reply":"2022-04-30T03:26:33.990924Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n# sqlContext=SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\ndf=sqlContext.createDataFrame(df)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:27:05.632505Z","iopub.execute_input":"2022-04-30T03:27:05.632951Z","iopub.status.idle":"2022-04-30T03:28:33.726343Z","shell.execute_reply.started":"2022-04-30T03:27:05.632875Z","shell.execute_reply":"2022-04-30T03:28:33.725398Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Constructing Feature  Vectors for Machine Learning","metadata":{}},{"cell_type":"code","source":"df.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:31:02.360462Z","iopub.status.idle":"2022-04-30T03:31:02.360815Z","shell.execute_reply.started":"2022-04-30T03:31:02.360621Z","shell.execute_reply":"2022-04-30T03:31:02.360644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vector Assembler + Encoding + Preprocessing","metadata":{}},{"cell_type":"code","source":"my_cols = df.select(['avg_price',\n 'sum_price',\n 'fashion_news_frequency',\n 'club_member_status',\n 'Active',\n])\n# my_final_data = my_cols.na.drop()\nfrom pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n                                OneHotEncoder,StringIndexer)\nfnf_indexer = StringIndexer(inputCol='fashion_news_frequency',outputCol='fnfIndex')\nfnf_encoder = OneHotEncoder(inputCol='fnfIndex',outputCol='fnfVec')\ncms_indexer = StringIndexer(inputCol='club_member_status',outputCol='cmsIndex')\ncms_encoder = OneHotEncoder(inputCol='cmsIndex',outputCol='cmsVec')\nActive_indexer = StringIndexer(inputCol='Active',outputCol='ActiveIndex')\nActive_encoder = OneHotEncoder(inputCol='ActiveIndex',outputCol='ActiveVec')\nassembler = VectorAssembler(inputCols=['sum_price',\n 'fnfVec',\n 'cmsVec',\n 'ActiveVec'],outputCol='features')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:31:05.800242Z","iopub.execute_input":"2022-04-30T03:31:05.800943Z","iopub.status.idle":"2022-04-30T03:31:05.988165Z","shell.execute_reply.started":"2022-04-30T03:31:05.800890Z","shell.execute_reply":"2022-04-30T03:31:05.987176Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Applying classification Algortihms","metadata":{}},{"cell_type":"code","source":"# from pyspark.ml.feature import  StringIndexer, VectorAssembler\n# categoricalColumns=['fashion_news_frequency','club_member_status','Active']\n# numericCols = ['sum_price']\n# assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n# assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nfrom pyspark.ml.classification import LogisticRegression\nlog_reg = LogisticRegression(featuresCol='features',labelCol='avg_price')\ntrain_data, test_data = df.randomSplit([0.7,.3])\nfit_model = log_reg.fit(train_data)\nresults = fit_model.transform(test_data)\n# from pyspark.ml.classification import LogisticRegression\n# lr = LogisticRegression(featuresCol = X_trainn, labelCol = y_train, maxIter=10)\n# lrModel = lr.fit(X_train)","metadata":{},"execution_count":null,"outputs":[]}]}